\section{Baseline Model}

\subsection*{Mathematical Framework}

For binary undercut success, we model:

\[
P(y_i = 1 \mid X_i) = \sigma(X_i^\top \beta) = \frac{1}{1 + e^{-X_i^\top \beta}},
\]

with log-odds 

\[
\log\left(\frac{p}{1-p}\right) = X_i^\top \beta.
\]

Class imbalance is handled via class weights:

\[
w_{\text{class}} = \frac{n}{2 n_{\text{class}}}.
\]

The weighted log-likelihood is

\[
L(\beta) = \sum_i w_i \left[ y_i \log p_i + (1 - y_i)\log(1 - p_i) \right].
\]

\subsection*{Training Setup}

Training uses a 70/30 stratified split, standardization, logistic regression, and class weighting. The model includes features such as gap, pace differential, tire ages, pit stop durations, tire age differential, gap per lap, and one-hot encoded circuit and year features. After encoding, the model uses roughly 40 inputs.

\subsection*{Performance}

On the test set, the model achieved accuracy of 0.76 (not meaningful due to imbalance), precision of 0.21, recall of 0.55, F1 score of 0.31, and AUC--ROC of 0.71, which is the most informative metric since it is threshold-independent and robust to imbalance. The model performs better than random chance (AUC--ROC $> 0.50$) and demonstrates predictive capability. Circuit characteristics dominate feature importance, with circuit IDs comprising most of the top features, validating the EDA finding that circuit is the dominant factor. Pit stop times (a\_pit\_ms) are the strongest non-circuit predictor, and year 2022 shows a strong negative effect, consistent with temporal variation observed in EDA.

\section{Addressing Class Imbalance}

Threshold tuning replaces the default 0.5 threshold with a value that maximizes F1, typically around 0.47, increasing recall to 0.65 but reducing precision to 0.23. A lower threshold of 0.3 increases recall further to 0.70 but reduces precision to 0.18. Random undersampling reduces the majority class to a 3:1 ratio relative to the minority, achieving precision of 0.17 and recall of 0.40, though this approach was detrimental to AUC--ROC, precision, and recall compared to the baseline.

Comparing methods shows that the baseline (balanced weights) yields precision of 0.21 and recall of 0.55 with F1 of 0.31; threshold tuning at 0.3 increases recall to 0.70 but reduces precision to 0.18 with F1 of 0.28; the optimal F1 threshold (0.47) yields precision of 0.23 and recall of 0.65 with F1 of 0.34; random undersampling yields precision of 0.17 and recall of 0.40 with F1 of 0.24. The optimal threshold approach provides the most balanced performance, but selection depends on whether false positives or false negatives are more costly in the application context.

\section{Conclusion}

We built a clean undercut attempts dataset for 2014--2024, applied physics-informed filtering (2-second gap threshold), performed detailed EDA, engineered domain-specific features (tire age differential, gap per lap), and constructed a baseline logistic regression model with proper class imbalance treatment. The final dataset contains 761 legitimate undercut attempts after filtering. The EDA strongly influenced modeling choices such as the gap filter, imbalance handling via class weights and threshold adjustments, and the use of circuit and tire age differential features.

The baseline model achieved an AUC--ROC of 0.71 on the test set, demonstrating predictive capability beyond random chance. Circuit characteristics dominate feature importance, with top-performing circuits (Circuit Gilles Villeneuve at 34.5\%, Albert Park at 26.1\%, Monaco at 25.0\%) achieving 2.5--3.5$\times$ higher success rates than the overall 10.1\% average. Pit stop times show the strongest linear correlation with success (pit\_ms: 0.14, a\_pit\_ms: 0.12), and weak linear relationships (all correlations $< 0.15$) suggest non-linear interactions dominate.

This model can support real-time strategic decisions, risk assessments, circuit-dependent analysis, and tire management optimization. Future improvements include SMOTE-based augmentation, XGBoost or LightGBM with imbalance-aware loss functions, cost-sensitive learning, interaction terms (circuit $\times$ pit\_time, gap $\times$ tire\_age\_diff), weather and temperature features, and nonlinear deep learning architectures. Given the dominance of circuit effects, hierarchical models that explicitly account for circuit-specific characteristics should be explored, potentially using circuit-specific intercepts or random effects to capture the substantial variation in success rates across tracks.

\bigskip

\noindent For full code and visualizations, refer to the Jupyter notebook \texttt{m3-coding.ipynb}.

